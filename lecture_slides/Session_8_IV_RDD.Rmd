---
output:
  slidy_presentation:
    self_contained: no
    css: template.css
    includes:
      after_body: footer.html
  beamer_presentation: default
---

## 

<div class="header-container first-slide-header">
  <img src="logo.jpg" class="logo" alt="University Logo">
  <img src="uibk_header1.png" class="header-image" alt="University Header">
  <h1>Lecture 8: Instrumental variables and RDD</h1> 
</div>



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(purrr)
library(patchwork)
library(ggpubr)
library(dagitty)
library(ggdag)
library(gganimate)
library(ggthemes)
library(Cairo)
library(modelsummary)
library(wooldridge)
library(fixest)

knitr::opts_chunk$set(fig.width = 5, fig.height = 3)

theme_set(
  theme_minimal(base_size = 10) +   # Adjust base font size
  theme(
    plot.title = element_text(size = 10, face = "bold"),  # Title font size and boldness
    axis.text = element_text(size = 8),                  # Axis text size
    axis.title = element_text(size = 10),                # Axis label size
    legend.title = element_text(size = 8),               # Legend title font size
    legend.text = element_text(size = 7)                 # Legend item font size
  )
)
```


## Recap

-   We've covered several methods for isolating causal effects!
-   Controlling for variables to close back doors (explain X and Y with the control, remove what's explained)
-   Matching on variables to close back doors (find treated and non-treated observations with identical values of controls)
-   Using a control group to control for time (before/after difference for treated and untreated, then difference them)

## Today

-   We've got TWO MORE METHODs to go deep on!
-   First, we'll be covering *instrumental variables*
    -   The basic idea is that we have some variable - the instrumental variable - that causes `X` but has no other open back doors!
-   Second, we'll cover *regression discontinuity designs*
    - Here the basic idea is to compare groups close to a discontinuity in a running variable (e.g. test score) that affects treatment assignment

## Natural Experiments

-   This calls back to our idea of trying to mimic an experiment without having an experiment. In fact, let's think about an actual randomized experiment.
-   We have some random assignment `R` that determines your `X`. So even though we have back doors between `X` and `Y`, we can identify `X -> Y`

```{r, dev='CairoPNG', echo=FALSE, fig.width=6,fig.height=3}
dag <- dagify(Y~X+W,
              X~W,
              X~R,
              coords=list(
                x=c(X=1,Y=2,W=1.5,R=0),
                y=c(X=1,Y=1,W=2,R=1)
              )) %>% tidy_dagitty()
ggdag_classic(dag,node_size=20) + 
  theme_dag_blank()
```

## Natural Experiments

-   The idea of instrumental variables is this:
-   What if we can find a variable that can take the place of R in the diagram despite not actually being something we randomized in an experiment?
-   If we can do that, we've clearly got a "natural experiment"
-   When we find a variable that can do that, we call it an "instrument" or "instrumental variable"
-   Let's call it `Z`

## Instrumental Variable

So, for `Z` take the place of `R` in the diagram, what do we need?

-   `Z` must be related to `X` (typically `Z -> X` but not always)
-   There must be *no open paths* from `Z` to `Y` *except for ones that go through `X`*

In other words "`Z` is related to `X`, and all the effect of `Z` on `Y` goes THROUGH `X`"

-   This implies no backdoors between `Z` and `Y` (unconfounded instrument)
-   and no alternative frontdoors (exclusion restriction)

## Instrumental Variable

-   This doesn't relieve us of the duty of identifying a causal effect by closing back doors
-   But it *moves* that duty from the endogenous variable to the instrument, which potentially is easier to identify
-   (and then adds on the additional requirement that there are also no open *front* doors from $Z$ to $Y$ except through $X$ )

## Instrumental Variable

How?

-   Explain `X` with `Z`, and keep only what *is* explained, `X'`
-   Explain `Y` with `Z`, and keep only what *is* explained, `Y'`
-   [If `Z` is logical/binary] Divide the difference in `Y'` between `Z` values by the difference in `X'` between `Z` values
-   [If `Z` is not logical/binary] Get the correlation between `X'` and `Y'`

## Estimation

-   We will be doing this mostly by hand today (until the end part) but most commonly this is estimated using *two stage least squares*
-   We basically just do what we described on the last slide:

1.  Use the instruments and controls to explain $X$ in the first stage
2.  Use the controls and the predicted (explained) part of $X$ in place of $X$ in the second stage
3.  (do some standard error adjustments)

Many ways to do this in R, we'll be doing 2SLS with `feols()` from **fixest**

## Graphically

&nbsp;

&nbsp;

```{r, echo=FALSE}
df <- data.frame(Z = as.integer(1:200>100),
                 W = rnorm(200)) %>%
  mutate(X = .5+2*W +2*Z+ rnorm(200)) %>%
  mutate(Y = -X + 4*W + 1 + rnorm(200),time="1") %>%
  group_by(Z) %>%
  mutate(mean_X=mean(X),mean_Y=mean(Y),YL=NA,XL=NA) %>%
  ungroup()

#Calculate correlations
before_cor <- paste("1. Raw data. Correlation between X and Y: ",round(cor(df$X,df$Y),3),sep='')
afterlab <- '6. The slope between points is the effect of X on Y.'

dffull <- rbind(
  #Step 1: Raw data only
  df %>% mutate(mean_X=NA,mean_Y=NA,time=before_cor),
  #Step 2: Add x-lines
  df %>% mutate(mean_Y=NA,time='2. What differences in X are explained by Z?'),
  #Step 3: X de-meaned 
  df %>% mutate(X = mean_X,mean_Y=NA,time="3. Remove everything in X not explained by Z"),
  #Step 4: Remove X lines, add Y
  df %>% mutate(X = mean_X,mean_X=NA,time="4. What differences in Y are explained by Z?"),
  #Step 5: Y de-meaned
  df %>% mutate(X = mean_X,Y = mean_Y,mean_X=NA,time="5. Remove everything in Y not explained by Z"),
  #Step 6: Raw demeaned data only
  df %>% mutate(X =  mean_X,Y =mean_Y,mean_X=NA,mean_Y=NA,YL=mean_Y,XL=mean_X,time=afterlab))

#Get line segments
endpts <- df %>%
  group_by(Z) %>%
  summarize(mean_X=mean(mean_X),mean_Y=mean(mean_Y))

p <- ggplot(dffull,aes(y=Y,x=X,color=as.factor(Z))) +
  geom_point() +
  geom_vline(aes(xintercept=mean_X,color=as.factor(Z))) +
  geom_hline(aes(yintercept=mean_Y,color=as.factor(Z))) +
  guides(color=guide_legend(title="Z")) +
  geom_segment(aes(x=ifelse(time==afterlab,endpts$mean_X[1],NA),
                   y=endpts$mean_Y[1],xend=endpts$mean_X[2],
                   yend=endpts$mean_Y[2]), size=1, color='blue') +
  scale_color_colorblind() +
  labs(title = 'X -> Y, With Binary Z as an Instrumental Variable \n{next_state}') +
  transition_states(time, transition_length=c(6,16,6,16,6,6), 
                    state_length=c(50,22,12,22,12,50), wrap=FALSE) +
  ease_aes('sine-in-out') +
  exit_fade() + 
  enter_fade() +
  theme(aspect.ratio = 0.5)  #


animate(p,nframes=175)
```

## Instrumental Variables

-   Notice that this whole process is like the *opposite* of controlling for a variable
-   We explain `X` and `Y` with the variable, but instead of tossing out what's explained, we ONLY KEEP what's explained!
-   Instead of saying "you're on a back door, I want to close you" we say "you have no back doors! I want my `X` to be just like you! I'm only keeping that part of `X` that's explained by you!"
-   Since `Z` has no back doors, the part of `X` explained by `Z` has no back doors to the part of `Y` explained by `Z`

## Imperfect Assignment

-   Let's apply one of the common uses of instrumental variables, which actually *is* when you have a randomized experiment
-   In normal circumstances, if we have an experiment and assign people with `R`, we just compare `Y` across values of `R`:

```{r, echo=TRUE}
df <- tibble(R = sample(c(0,1),5000,replace=T)) %>%
  mutate(X = R, Y = 5*X + rnorm(5000))
#The truth is a difference of 5
df %>% group_by(R) %>% summarize(Y=mean(Y))
```

## Imperfect Assignment

-   But what happens if you run a randomized experiment and assign people with `R`, but not everyone does what you say? Some "treated" people don't get the treatment, and some "untreated" people might get it
-   When this happens, we can't just compare `Y` across `R`
-   But `R` is still a valid instrument!

## Imperfect Assignment

In a setting with a binary instrument and a binary treatment, we can distinguish between four types:

- **Always takers**:  take the treatment irrespective of the value of the instrument
- **Compliers**: take the treatment if R=1 and don't take the treatment if R=0 
- **Never takers**:  do not take the treatment irrespective of the value of the instrument
- **Defiers**: take the treatment if R=0 and don't take the treatment if R=1 


## Imperfect Assignment

```{r, echo=TRUE}
df <- tibble(R = sample(c(0,1),5000,replace=T)) %>%
  #We tell them whether or not to get treated
  mutate(X = R) %>%
  #But some of them don't listen! 50% never take the treatment
  mutate(X = ifelse(runif(5000) > .5,0,R)) %>%
  mutate(Y = 5*X + rnorm(5000))
#The truth is a difference of 5
df %>% group_by(R) %>% summarize(Y=mean(Y))
```

*Exercise*: Change the code to create always takers and defiers.

## Imperfect Assignment

-   So let's do IV (instrumental variables); `R` is the IV.

```{r, echo=TRUE}
iv <- df %>% group_by(R) %>% summarize(Y = mean(Y), X = mean(X))
iv
#Remember, since our instrument is binary, we want the difference in the means
(iv$Y[2] - iv$Y[1])/(iv$X[2]-iv$X[1])
#Truth is 5!
```

## Perform IV estimation using two-state least squares
```{r, echo=TRUE}
# First Stage: Predicting X using R
first_stage <- lm(X ~ R, data = df)

# Add predicted values to the dataset
df <- df %>% mutate(X_hat = predict(first_stage, df))

# Second Stage: Regressing Y on predicted X_hat
second_stage <- lm(Y ~ X_hat, data = df, se='hetero')

# One-step using feols
iv_model <- feols(Y ~ 1 | X ~ R, data = df, se='hetero')
```

## Perform IV estimation using two-state least squares
```{r, echo=TRUE}
msummary(list(first_stage, second_stage, iv_model), stars = FALSE, gof_omit = 'AIC|BIC|Lik|F|R2|RMSE')
```

## Another Example

-   Justifying that an IV has no back doors can be hard!
-   Usually things aren't as clean-cut as having actual randomization
-   And sometimes we may have to add controls in order to justify the IV
-   Think hard - are there really no other paths from `Z` to `Y`?
-   This will often require *detailed contextual knowledge* of the data generating process

## Practice

-   Does the price of cigarettes affect smoking? Get AER package and data(CigarettesSW). Examine with help().
-   Get JUST thecigarette taxes `cigtax` from `taxs-tax`
-   Draw a causal diagram using `packs`, `price`, `cigtax`, and some back door `W`. What might `W` be?
-   Adjust `price` and `cigtax` for inflation: divide them by `cpi`
-   Explain `price` and `packs` with `cigtax` using `cut(,breaks=7)` for `cigtax`
-   Get correlation between the explained parts and plot the explained parts - does price reduce packs smoked?

## Practice Answers

```{r, echo=TRUE}
library(AER)
data(CigarettesSW)

CigarettesSW <- CigarettesSW %>%
  mutate(cigtax = taxs-tax) %>%
  mutate(price = price/cpi,
         cigtax = cigtax/cpi) %>%
  group_by(cut(cigtax,breaks=7)) %>%
  summarize(priceexp = mean(price),
         packsexp = mean(packs)) %>%
  ungroup()

cor(CigarettesSW$priceexp,CigarettesSW$packsexp)
```

## Practice Answers Plot

```{r, echo=TRUE, fig.width=6,fig.height=4}
plot(CigarettesSW$priceexp,CigarettesSW$packsexp)
```

## Practice Diagram Answers

```{r, dev='CairoPNG', echo=FALSE, fig.width=6,fig.height=4}
dag <- dagify(price~cigtax+W,
              packs~price+W,
              coords=list(
                x=c(packs=2,price=1,cigtax=0,W=1.5),
                y=c(packs=1,price=1,cigtax=1,W=2)
              )) %>% tidy_dagitty()
ggdag_classic(dag,node_size=20) + 
  theme_dag_blank()
```

## Practice - Doing it with Regression!

-   Common 2SLS estimators: `ivreg` in **AER**, `iv_robust` in **estimatr**, and `feols()` in **fixest**. We'll use the latter since it's fast easy to combine with fixed effects and all kinds of error adjustments

```{r, echo = TRUE, eval = FALSE}
m <- feols(Y ~ controls | X ~ Z, data = data)
m <- feols(Y ~ controls | fixed_effects | X ~ Z, data = data, se = 'hetero')
```

## Practice - Doing it with Regression

-   Reload the cigarette data and skip the summarize step
-   Run our cigarette analysis first doing 2SLS by hand - use `lm()` to run the first stage, then replace `price` with `predict(m)` in the second stage
-   Then use `feols()` to do the same (use 1 to indicate no controls). Coefficients should be the same but the standard errors will be corrected in the `feols()` version!
-   Show both results in `msummary()`

## Practice - Doing it with Regression

```{r, echo = TRUE}
data(CigarettesSW)

CigarettesSW <- CigarettesSW %>%
  mutate(cigtax = taxs-tax) %>%
  mutate(price = price/cpi,
         cigtax = cigtax/cpi)
first_stage <- lm(price~cigtax, data = CigarettesSW)
second_stage <- lm(packs ~ predict(first_stage), data = CigarettesSW)
package <- feols(packs ~ 1 | price ~ cigtax, data = CigarettesSW)
```

## Practice - Doing it with Regression

```{r, echo = TRUE}
msummary(list(second_stage, package), stars = TRUE, gof_omit = 'AIC|BIC|Lik|F|R2')
```

## Regression Discontinuity

- For regression discontinuity to work, we need the Treatment to be assigned based on a *cutoff* of what's called a "running variable"
- For example, imagine we want to know the effects of being in a Gifted and Talented (GATE) program on your adult earnings
- Being admitted to the program is based on your test score (running variable)
- If you score above 75, you're in the program. 75 or below, you're out!

## Regression Discontinuity

- Notice that the y-axis here is *In GATE*, not the outcome

```{r, echo=FALSE, eval=TRUE, fig.width=7, fig.height=5.5}
rdd <- tibble(test = runif(300)*100) %>%
  mutate(GATE = test >= 75,
         above = test >= 75) %>%
  mutate(earn = runif(300)*40+10*GATE+test/2)

ggplot(rdd,aes(x=test,y=GATE))+geom_point()+
  geom_vline(aes(xintercept=75),col='red')+
  theme_pubr()
  labs(x='Test Score',
       y='In GATE')
```

## Regression Discontinuity

- Here's how it look when we look at the actual outcome

```{r, echo=FALSE, eval=TRUE, fig.width=7, fig.height=5.5}
ggplot(rdd,aes(x=test,y=earn,color=GATE))+geom_point()+
  theme_pubr() +
  geom_vline(aes(xintercept=75),col='red')+
  labs(x='Test Score',
       y='Earnings')
```


## Regression Discontinuity

- Now, we have a bit of a problem!
- If we look at the relationship between treatment and going to college, we'll be picking up the fact that higher test scores make you more likely to go to college anyway

```{r, dev='CairoPNG', echo=FALSE, fig.width=6,fig.height=4}
dag <- dagify(earn~GATE+Test,
              GATE~Test,
              coords=list(
                x=c(earn=3,GATE=1,Test=2.5),
                y=c(earn=1,GATE=1.5,Test=2)
              )) %>% tidy_dagitty()
ggdag_classic(dag,node_size=20) + 
  theme_dag_blank()
```

## Regression Discontinuity

- Except, that's not actually what the diagram looks like! Test only affects GATE to the extent that it makes you be above the 90 cutoff!

```{r, dev='CairoPNG', echo=FALSE, fig.width=6,fig.height=4}
dag <- dagify(earn~GATE+Test,
              Above~Test,
              GATE~Above,
              coords=list(
                x=c(earn=3,GATE=1,Test=2.5,Above=1.75),
                y=c(earn=1,GATE=1.5,Test=2,Above=1.75)
              )) %>% tidy_dagitty()
ggdag_classic(dag,node_size=20) + 
  theme_dag_blank()
```

## Regression Discontinuity

- What can we do with that information?
- Well, imagine that we looked at the area *just around* the cutoff
- Say, the cutoff is 75, so we look at 73 to 77
- *Within that group*, it's basically random whether you fall on one side of the line or another

## Regression Discontinuity

- Someone with a 75 is, on average, almost exactly the same as someone with a 76, except that one got the treatment and the other didn't!
- Heck, that tiny test score difference could be due to just having a bad day before the test
- So we have two groups - the just-barely-missed-outs and the just-barely-made-its, that are basically exactly the same except that one happened to get treatment
- A perfect description of what we're looking for in a control group!

## Regression Discontinuity

- So we look directly around the cutoff, and compare just below to just above.
- This is our way of controlling for test score and closing the `GATE <- Above <- Test -> earn` back door
- Why not just control for `Test` in the normal way?
- Because if we really think that, right around the cutoff, it's random whether you're on one side or the other, we don't just close the `Test` back door, we have effectively random assignment, like an experiment!
- We're not just closing the `Test` back door, we're closing *all* back doors

## In Practice

```{r, echo=TRUE, eval=FALSE}
rdd.data <- tibble(test = runif(1000)*100) %>%
  mutate(GATE = test >= 75) %>% mutate(earn = runif(1000)*40+10*GATE+test/2)
#Choose a "bandwidth" of how wide around the cutoff to look (arbitrary in our example)
#Bandwidth of 2 with a cutoff of 75 means we look from 75-2 to 75+2
bandwidth <- 2
#Just look within the bandwidth
rdd <- rdd.data %>% filter(abs(75-test) < bandwidth) %>%
  #Create a variable indicating we're above the cutoff
  mutate(above = test >= 75) %>%
  #And compare our outcome just below the cutoff to just above
  group_by(above) %>% summarize(earn = mean(earn))
rdd
#Our effect looks just about right (10 is the truth)
rdd$earn[2] - rdd$earn[1]
```

```{r, echo=FALSE, eval=TRUE}
set.seed(1000)
rdd.data <- tibble(test = runif(1000)*100) %>%
  mutate(GATE = test >= 75) %>% mutate(earn = runif(1000)*40+10*GATE+test/2)
#Choose a "bandwidth" of how wide around the cutoff to look (arbitrary in our example)
#Bandwidth of 2 with a cutoff of 75 means we look from 75-2 to 75+2
bandwidth <- 2
#Just look within the bandwidth
rdd <- rdd.data %>% filter(abs(75-test) < bandwidth) %>%
  #Create a variable indicating we're above the cutoff
  mutate(above = test >= 75) %>%
  #And compare our outcome just below the cutoff to just above
  group_by(above) %>% summarize(earn = mean(earn))
rdd
#Our effect looks just about right (10 is the truth)
rdd$earn[2] - rdd$earn[1]
```

## Graphically

```{r, echo=FALSE, fig.width=4, fig.height=4, out.width="50%", fig.align = "center"}
library(gifski)
library(tidyverse)
library(gganimate)

# The code to generate and save the animation as a GIF
df <- data.frame(xaxisTime=runif(300)*20) %>%
  mutate(Y = .2*xaxisTime + 3*(xaxisTime > 10) - .1*xaxisTime*(xaxisTime > 10) + rnorm(300),
         state = "1",
         groupX = floor(xaxisTime) + .5,
         groupLine = floor(xaxisTime),
         cutLine = rep(c(9, 11), 150)) %>%
  group_by(groupX) %>%
  mutate(mean_Y = mean(Y)) %>%
  ungroup() %>%
  arrange(groupX)

dffull <- rbind(
  df %>% mutate(groupLine = NA, cutLine = NA, mean_Y = NA, state = '1. Start with raw data.'),
  df %>% mutate(cutLine = NA, state = '2. What differences in Y are explained by Running Variable?'),
  df %>% mutate(Y = mean_Y, state = "3. Keep only what's explained by the Running Variable."),
  df %>% mutate(mean_Y = ifelse(xaxisTime > 9 & xaxisTime < 11, mean_Y, NA),
                Y = ifelse(xaxisTime > 9 & xaxisTime < 11, mean_Y, NA), 
                groupLine = NA, state = "4. Focus just on what happens around the cutoff."),
  df %>% mutate(mean_Y = ifelse(xaxisTime > 9 & xaxisTime < 11, mean_Y, NA),
                Y = ifelse(xaxisTime > 9 & xaxisTime < 11, mean_Y, NA), 
                groupLine = NA, state = "5. The jump at the cutoff is the effect of treatment.")
)

p <- ggplot(dffull, aes(y = Y, x = xaxisTime)) +
  geom_point() +
  geom_vline(aes(xintercept = 10), linetype = 'dashed') +
  geom_point(aes(y = mean_Y, x = groupX), color = "red", size = 2) +
  geom_vline(aes(xintercept = groupLine)) +
  geom_vline(aes(xintercept = cutLine)) +
  geom_segment(aes(x = 10, xend = 10,
                   y = ifelse(state == '5. The jump at the cutoff is the effect of treatment.',
                              filter(df, groupLine == 9)$mean_Y[1], NA),
                   yend = filter(df, groupLine == 10)$mean_Y[1]), 
               size = 1.5, color = 'blue') +
  scale_color_colorblind() +
  scale_x_continuous(breaks = c(5, 15), label = c("Untreated", "Treated")) +
  xlab("Running Variable") +
  labs(title = 'The Effect of Treatment on Y using Regression Discontinuity \n{next_state}') +
  transition_states(state, transition_length = c(6, 16, 6, 16, 6), 
                    state_length = c(50, 22, 12, 22, 50), wrap = FALSE) +
  ease_aes('sine-in-out') +
  exit_fade() + enter_fade()+
  theme(
    plot.title = element_text(size = 8),  
    axis.title = element_text(size = 6),   
    legend.title = element_text(size = 6),
    legend.text = element_text(size = 5),
    plot.margin = margin(t = 30, r = 10, b = 10, l = 10) # Increase top margin
  )

animate(p, nframes = 175, renderer = gifski_renderer("rdd_animation.gif"))


```

## Example: Corporate Social Responsibility

- Corporate Social Responsibility (CSR) is when corporations engage in the kind of behavior that nonprofits usually do - community outreach, charity, etc.
- Is this good for the corporation? Or would it make more sense to just send the money they spend to actual nonprofits if they just want to do good?
- This is a causal question

## Example: Corporate Social Responsibility

- Convenient for our purposes, CSR policies are voted on by shareholder boards
- If a board votes 49% in favor, it fails. 51% in favor? It passes!
- Sounds like a regression discontinuity to me!
- "Close votes" is a common application of regression discontinuity

## Example: Corporate Social Responsibility

- So how do CSR policy announcements affect stock prices?

```{r, dev='CairoPNG', echo=FALSE, fig.width=6,fig.height=5}
dag <- dagify(price~vote+CSR,
              win~vote,
              CSR~win,
              coords=list(
                x=c(price=3,CSR=1,vote=2.5,win=1.75),
                y=c(price=1,CSR=1.5,vote=2,win=1.75)
              )) %>% tidy_dagitty()
ggdag_classic(dag,node_size=20) + 
  theme_dag_blank()
```

## Example: Corporate Social Responsbility

- Caroline Flammer studies this topic
- Looking at the "abnormal return" (stock price return minus what's expected given the market) comparing CSR votes that just won vs. CSR votes that just lost
- So what should we do?
- Focus just around the cutoff and compare abnormal returns just above and just below.

## Example: Corporate Social Responsibility

![Flammer (2015) Management Science](Lecture_12_Flammer.png)

## Example: Corporate Social Responsibility

- Looks like stock returns increase by about .02, comparing CSRs that just lost to just won!
- Seems like the market likes seeing those CSRs and values them
- And all those things that we might expect to correlate with both stock price growth and CSRs - tech-savvy, youthful leadership, etc., we've closed those back doors too!

## Balance

- Have we really closed those back doors?
- One thing that's so great about RDD is that, since it's basically random whether you're on one side of the cutoff or another, there shouldn't be other back doors
- It's a form of within variation that's *so narrow* it basically closes everything
- We can check this by seeing if other variables differ on either side of the line
- This is our way of testing our diagram - if our diagram is true, then `above` should have no relationship with any back door variable after focusing around the cutoff

## Balance

```{r, echo=TRUE}
rdd.data <- tibble(test = runif(500)*100) %>%
  mutate(backdoor=rnorm(500)+test/50) %>% mutate(GATE = test + backdoor >= 75) %>%
  mutate(earn = runif(500)*40+10*GATE+5*backdoor+test/2)
bandwidth <- 2
rdd <- rdd.data %>% filter(abs(75-test) < bandwidth) %>%
  #Create a variable indicating we're above the cutoff
  mutate(above = test >= 75) %>%
  #And compare our outcome just below the cutoff to just above
  group_by(above) %>% summarize(backdoor = mean(backdoor))
rdd
#Not a lot of difference!
rdd$backdoor[2] - rdd$backdoor[1]
```

## Balance

- Notice there's NO real difference here, indicating that we've closed that back door

```{r, echo=FALSE, fig.width=7,fig.height=5}
rdgrph <- rdd.data %>% 
  mutate(bandwidth = abs(75-test) <= 2,
         above = test >= 75) %>%
  group_by(cut(test,breaks=(0:49*2+1))) %>%
  mutate(meanback = mean(backdoor)) %>%
  ungroup()

ggplot(filter(rdgrph,bandwidth==1),aes(x=test,y=backdoor))+geom_point(col='blue')+
  geom_vline(aes(xintercept=75),col='red')+
  geom_vline(aes(xintercept=73),col='red',linetype='dashed',alpha=.6)+
  geom_vline(aes(xintercept=77),col='red',linetype='dashed',alpha=.6)+
  geom_point(data=filter(rdgrph,bandwidth==0),aes(x=test,y=backdoor),alpha=.3)+
  geom_step(data=rdgrph,aes(x=test,y=meanback),col='red',size=1)+
  theme_pubr()
  labs(x='Test Score',
      y='Backdoor Variable')
```

## Summing Up

- We've covered five main methods of making comparisons as close as possible
- **Controlling and matching** both take a set of measured variables and adjust so you're looking at variation within those variables
- **Difference-in-difference** takes a chosen comparison group and uses it to adjust for changes over time in your treated group of interest
- **Instrumental variables** induce *good* variation in the endogenous variables
- **Regression discontinuity** uses a cutoff in a running variable to identify a treated and nontreated group that are basically randomly assigned

## Practice

- Does winning help your party stay in power *30 years* later?
- Install and load the `politicaldata` package, and load `data(house_results)`
- Create tibbles `hr76` and `hr16` with only 1976 and 2016
- Create `repadv76` equal to `rep` vote minus `dem` for 1976, and filter only to those with `!is.na(repadv75)`
- Create `repwins16` equal to `rep > dem` for 2016, and filter `!is.na(repwins16)`
- `select()` only `district`,`repadv76`, `repwins16`, and `inner_join()` the two data sets
- Compare `repwins16` mean above and below `repadv76=0` with a bandwidth of .04

## Practice Answers

```{r, echo=TRUE}
# install the remotes package if it's not already
if (!requireNamespace("remotes", quietly = TRUE)) {
  install.packages("remotes")
}

# install dev version of politicaldata from github
remotes::install_github("elliottmorris/politicaldata")

# load the politicaldata package
library(politicaldata)

data(house_results)

hr76 <- filter(house_results,year==1976) %>%
  mutate(repadv76 = rep - dem) %>%
  filter(!is.na(repadv76)) %>%
  select(district,repadv76)

hr16 <- filter(house_results,year==2016) %>%
  mutate(repwins16 = rep > dem) %>%
  filter(!is.na(repwins16)) %>%
  select(district,repwins16)

fulldata <- inner_join(hr76,hr16)
bandwidth <- .04 

fulldata %>% filter(abs(repadv76-0)<=.04) %>%
  mutate(above = repadv76 > 0) %>%
  group_by(above) %>% summarize(repwins16=mean(repwins16))
```


